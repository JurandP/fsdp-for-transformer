#!/bin/bash
#SBATCH --job-name=submit_begin
#SBATCH --output=logs/training_output_begin.log
#SBATCH --error=logs/training_error_begin.log 
#SBATCH --nodes=1 
#SBATCH --ntasks=1 
#SBATCH --ntasks-per-node=1 
#SBATCH --gpus=1
#SBATCH --partition=###
#SBATCH --account=###
#SBATCH --mem=32GB 
#SBATCH --time=00:10:00 

# Activate your Python environment
source .venv/bin/activate


export OMP_NUM_THREADS=4  
export MASTER_PORT=12399 
export WORLD_SIZE=${SLURM_NTASKS}

master_addr=$(scontrol show hostnames "$SLURM_NODELIST" | head -n 1)
export MASTER_ADDR=$master_addr

echo "NODELIST=${SLURM_NODELIST}"
echo "WORLD_SIZE=${SLURM_NTASKS}"
echo "MASTER_ADDR=${MASTER_ADDR}"

# Run the script using torchrun for distributed training

python3 initial_main.py

# Deactivate the environment
deactivate
